{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":837,"status":"ok","timestamp":1720450001366,"user":{"displayName":"Giacomo B","userId":"01919569727522971924"},"user_tz":-120},"id":"sOkewLosq0VU"},"outputs":[],"source":["import os\n","import json\n","from itertools import chain\n","from hashlib import md5\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.semi_supervised import LabelSpreading\n","from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import loguniform"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset preparation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load LFW dataset face embeddings\n","\n","if os.path.exists(\"../data/lfw_facenet_embeddings.parquet\"):\n","    # try local path\n","    embedding_df = pd.read_parquet(\"../data/lfw_facenet_embeddings.parquet\")\n","else:\n","    # download from Hugging Face\n","    embedding_df = pd.read_parquet(\"hf://datasets/lajota13/lfw_facenet_embeddings/lfw_facenet_embeddings.parquet\")\n","    embedding_df.to_parquet(\"../data/lfw_facenet_embeddings.parquet\")\n","\n","embedding_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1772,"status":"ok","timestamp":1720450006044,"user":{"displayName":"Giacomo B","userId":"01919569727522971924"},"user_tz":-120},"id":"yyqSDr-nq4R2"},"outputs":[],"source":["# parse celebs names\n","\n","embedding_df[\"name\"] = embedding_df[\"label\"].str.replace(\"_\", \" \")\n","embedding_df.drop(columns=[\"label\"], inplace=True)\n","\n","embedding_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":320,"status":"ok","timestamp":1720450008813,"user":{"displayName":"Giacomo B","userId":"01919569727522971924"},"user_tz":-120},"id":"Ob8AKfRsrZi1"},"outputs":[],"source":["# load seasonal color analysis annotations\n","\n","with open(\"../data/celebrities.json\") as fid:\n","  annotations = json.load(fid)\n","annotations_df = pd.DataFrame(list(chain(*[[{\"name\": c, \"season\":  season} for c in celebs] for season, celebs in annotations.items()])))\n","annotations_df[\"macroseason\"] = annotations_df[\"season\"].str.extract(\"[a-z]+-([a-z]+)-[a-z]+\")\n","\n","annotations_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1720450012722,"user":{"displayName":"Giacomo B","userId":"01919569727522971924"},"user_tz":-120},"id":"w7TntspksNq1"},"outputs":[],"source":["# associate annotations to embeddings\n","\n","embedding_annotated_df = embedding_df.merge(annotations_df, on=\"name\", how=\"left\")\n","macrolabel_map_df = pd.DataFrame(\n","    {\n","        \"macrolabel\": [0, 1, 2, 3],\n","        \"macroseason\": [\"winter\", \"summer\", \"spring\", \"autumn\"]\n","    }\n",")\n","embedding_annotated_df = embedding_annotated_df.merge(macrolabel_map_df, on=\"macroseason\", how=\"left\")\n","embedding_annotated_df[\"macrolabel\"] = embedding_annotated_df[\"macrolabel\"].fillna(-1)\n","embedding_annotated_df[\"macrolabel\"] = embedding_annotated_df[\"macrolabel\"].astype(int)\n","\n","embedding_annotated_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_annotated_df[\"macrolabel\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# splitting dataset into labeled and unlabeled records (in order to sample the test set \n","# from the labeled portion of the dataset)\n","\n","labeled_df = embedding_annotated_df.loc[embedding_annotated_df[\"macrolabel\"] != -1, :].copy()\n","unlabeled_df = embedding_annotated_df.loc[embedding_annotated_df[\"macrolabel\"] == -1, :].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# in order to easily sample the test set in a reproducible manner, the names md5 hashes can be exploited\n","\n","labeled_df[\"name\"].apply(\n","    lambda s: int(md5(s.encode(\"utf-8\")).hexdigest()[-2:], 16)\n",").hist()\n","\n","# display splits percentange\n","labeled_df[\"split\"] = labeled_df[\"name\"].apply(\n","    lambda s: \"train\" if int(md5(s.encode(\"utf-8\")).hexdigest()[-2:], 16) < 195 else \"test\"\n",")\n","_ = labeled_df[\"split\"].value_counts() / len(labeled_df)\n","_.name = \"percentage\"\n","print(_)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build one view per each split, merging the unlabeled portion in the training set\n","\n","train_df = pd.concat(\n","    [\n","        labeled_df.loc[labeled_df[\"split\"] == \"train\", :].drop(columns=\"split\"),\n","        unlabeled_df\n","    ],\n","    axis=0\n",")\n","test_df = labeled_df.loc[labeled_df[\"split\"] == \"test\", :].drop(columns=\"split\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training set label distribution\n","print(train_df[\"macrolabel\"].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test set label distribution\n","print(test_df[\"macrolabel\"].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build numpy arrays representing the dataset splits\n","\n","X_train = np.vstack(train_df[\"embedding\"].tolist())\n","y_train = train_df[\"macrolabel\"].values.astype(np.int8)\n","X_test = np.vstack(test_df[\"embedding\"].tolist())\n","y_test = test_df[\"macrolabel\"].values.astype(np.int8)"]},{"cell_type":"markdown","metadata":{},"source":["## Label propagation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N = 20\n","rs = RandomizedSearchCV(\n","    estimator=LabelSpreading(kernel=\"knn\"), \n","    param_distributions={\n","        \"n_neighbors\": range(5, 20),\n","        \"alpha\": loguniform(0.2, 0.9)\n","    },\n","    n_iter=N,\n","    scoring=lambda ls, X, y: ls.score(X[y > -1], y[y > -1]), \n","    random_state=42\n",")\n","\n","rs.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Best score: {rs.best_score_}\")\n","ls =  rs.best_estimator_\n","y_pred = ls.predict(X_test)\n","ConfusionMatrixDisplay.from_predictions(y_pred=y_pred, y_true=y_test)\n","print(classification_report(y_pred=y_pred, y_true=y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9995,"status":"ok","timestamp":1720450062259,"user":{"displayName":"Giacomo B","userId":"01919569727522971924"},"user_tz":-120},"id":"S0b4VPE02aXB"},"outputs":[],"source":["# dump propagated labels\n","\n","unlabeled_df[\"macrolabel\"] = (ls.predict(np.vstack(unlabeled_df[\"embedding\"].tolist()))).astype(np.int8)\n","unlabeled_df = unlabeled_df.drop(columns=\"macroseason\").merge(macrolabel_map_df, on=\"macrolabel\", how=\"left\")\n","label_propagated_df = pd.concat([labeled_df, unlabeled_df])\n","label_propagated_df.to_parquet(\"../data/lfw_facenet_embeddings_label_propagated.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save embedding and labels for visualization in the TF embedding projector (https://projector.tensorflow.org/)\n","LOGDIR = \"logs\"\n","if not os.path.isdir(LOGDIR):\n","    os.mkdir(LOGDIR)\n","\n","np.savetxt(\n","    os.path.join(LOGDIR, \"embeddings.tsv\"), \n","    np.vstack(label_propagated_df[\"embedding\"].tolist()), \n","    delimiter=\"\\t\"\n",")\n","\n","label_propagated_df[[\"macroseason\", \"name\"]].to_csv(\n","    os.path.join(LOGDIR, \"metadata.tsv\"), \n","    sep=\"\\t\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO8YSlQ4HBdIfFIpP6sjCvz","mount_file_id":"1mhP4RKwYGxYYAGs_sTyY0d5R5WHTi6_s","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
